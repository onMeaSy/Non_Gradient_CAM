{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ee2f528",
   "metadata": {},
   "source": [
    "What they do is replace every pixel that needs to be removed with a weighted average of it's neighbours. Since some of it's \n",
    "neighbours might also need to be removed, we get system of linear equations that we have to solve, to find the new values of the pixels we want to replace.\n",
    "\n",
    "â€‹\n",
    "\n",
    "A custom single metric that is a combination of Least Relevant First and Most Relevant First:\n",
    "\n",
    "(Least Relevant First - Most Relevant First) / 2, accross different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17816249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import cv2\n",
    "import io\n",
    "import requests\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58770ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from road import ROADCombined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b6ec37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from EigenCAM import EigenCAM\n",
    "from ScoreCAM import ScoreCAM\n",
    "from AblationCAM import AblationCAM\n",
    "from model_tragets import ClassifierOutputTarget,ClassifierOutputSoftmaxTarget\n",
    "from functions import show_cam_on_image\n",
    "import functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5ecc0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the metrics on top of the CAM : \n",
    "def visualize_score(visualization, score, name, percentiles):\n",
    "    visualization = cv2.putText(visualization, name, (10, 20), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1, cv2.LINE_AA)\n",
    "    visualization = cv2.putText(visualization, \"(Least first - Most first)/2\", (10, 40), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255,255,255), 1, cv2.LINE_AA)\n",
    "    visualization = cv2.putText(visualization, f\"Percentiles: {percentiles}\", (10, 55), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1, cv2.LINE_AA)    \n",
    "    visualization = cv2.putText(visualization, \"Remove and Debias\", (10, 70), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1, cv2.LINE_AA) \n",
    "    visualization = cv2.putText(visualization, f\"{score:.5f}\", (10, 85), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1, cv2.LINE_AA)    \n",
    "    return visualization\n",
    "\n",
    "def benchmark(input_tensor, target_layers, eigen_smooth=False, aug_smooth=False, category=281):\n",
    "    methods = [(\"EigenCAM\", EigenCAM(model=model, target_layers=target_layers)),\n",
    "               (\"AblationCAM\", AblationCAM(model=model, target_layers=target_layers)),\n",
    "               (\"ScoreCAM\", ScoreCAM(model=model, target_layers=target_layers))]\n",
    "\n",
    "    cam_metric = ROADCombined(percentiles=[20, 40, 60, 80])\n",
    "    targets = [ClassifierOutputTarget(category)]\n",
    "    metric_targets = [ClassifierOutputSoftmaxTarget(category)]\n",
    "    \n",
    "    visualizations = []\n",
    "    percentiles = [10, 50, 90]\n",
    "    for name, cam_method in methods:\n",
    "        with cam_method:\n",
    "            attributions = cam_method(input_tensor=input_tensor, \n",
    "                                      targets=targets, eigen_smooth=eigen_smooth, aug_smooth=aug_smooth)\n",
    "        attribution = attributions[0, :]    \n",
    "        scores = cam_metric(input_tensor, attributions, metric_targets, model)\n",
    "        score = scores[0]\n",
    "        visualization = show_cam_on_image(cg, attribution, use_rgb=True)\n",
    "        print(name)\n",
    "        print(score)\n",
    "        print(\" \")\n",
    "        #visualization = visualize_score(visualization, score, name, percentiles)\n",
    "        visualizations.append(visualization)\n",
    "    return Image.fromarray(np.hstack(visualizations))\n",
    "\n",
    "\n",
    "def preprocess_image(\n",
    "    img: np.ndarray, mean=[\n",
    "        0.5, 0.5, 0.5], std=[\n",
    "            0.5, 0.5, 0.5]) -> torch.Tensor:\n",
    "    preprocessing = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "    return preprocessing(img.copy()).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6132c8b9",
   "metadata": {},
   "source": [
    "Resnet50 model and [model.layer4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f414df5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#model.cuda()\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#input_tensor = input_tensor.cuda()\u001b[39;00m\n\u001b[0;32m     22\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m \u001b[43mbenchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maug_smooth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [13], line 28\u001b[0m, in \u001b[0;36mbenchmark\u001b[1;34m(input_tensor, target_layers, eigen_smooth, aug_smooth, category)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, cam_method \u001b[38;5;129;01min\u001b[39;00m methods:\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cam_method:\n\u001b[1;32m---> 28\u001b[0m         attributions \u001b[38;5;241m=\u001b[39m \u001b[43mcam_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meigen_smooth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maug_smooth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maug_smooth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     attribution \u001b[38;5;241m=\u001b[39m attributions[\u001b[38;5;241m0\u001b[39m, :]    \n\u001b[0;32m     31\u001b[0m     scores \u001b[38;5;241m=\u001b[39m cam_metric(input_tensor, attributions, metric_targets, model)\n",
      "File \u001b[1;32m~\\Documents\\pytorch-grad-cam\\cams\\utils\\base.py:188\u001b[0m, in \u001b[0;36mBaseCAM.__call__\u001b[1;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aug_smooth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_augmentation_smoothing(\n\u001b[0;32m    186\u001b[0m         input_tensor, targets, eigen_smooth)\n\u001b[1;32m--> 188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\pytorch-grad-cam\\cams\\utils\\base.py:95\u001b[0m, in \u001b[0;36mBaseCAM.forward\u001b[1;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[0;32m     84\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# In most of the saliency attribution papers, the saliency is\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# computed with a single target layer.\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Commonly it is the last convolutional layer.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# use all conv layers for example, all Batchnorm layers,\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# or something else.\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m cam_per_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_cam_per_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_multi_layers(cam_per_layer)\n",
      "File \u001b[1;32m~\\Documents\\pytorch-grad-cam\\cams\\utils\\base.py:127\u001b[0m, in \u001b[0;36mBaseCAM.compute_cam_per_layer\u001b[1;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(grads_list):\n\u001b[0;32m    125\u001b[0m     layer_grads \u001b[38;5;241m=\u001b[39m grads_list[i]\n\u001b[1;32m--> 127\u001b[0m cam \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cam_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mtarget_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mlayer_activations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mlayer_grads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m                         \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m cam \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(cam, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    134\u001b[0m scaled \u001b[38;5;241m=\u001b[39m scale_cam_image(cam, target_size)\n",
      "File \u001b[1;32m~\\Documents\\pytorch-grad-cam\\cams\\utils\\EigenCAM.py:20\u001b[0m, in \u001b[0;36mEigenCAM.get_cam_image\u001b[1;34m(self, input_tensor, target_layer, target_category, activations, grads, eigen_smooth)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_cam_image\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     14\u001b[0m                   input_tensor,\n\u001b[0;32m     15\u001b[0m                   target_layer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m                   grads,\n\u001b[0;32m     19\u001b[0m                   eigen_smooth):\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_2d_projection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\pytorch-grad-cam\\cams\\utils\\functions.py:69\u001b[0m, in \u001b[0;36mget_2d_projection\u001b[1;34m(activation_batch)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_2d_projection\u001b[39m(activation_batch):\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# TBD: use pytorch batch svd implementation\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m     activation_batch[\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivation_batch\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     70\u001b[0m     projections \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m activations \u001b[38;5;129;01min\u001b[39;00m activation_batch:\n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "url = requests.get('https://images.pexels.com/photos/45201/kitty-cat-kitten-pet-45201.jpeg?auto=compress&cs=tinysrgb&w=1600',stream=True)\n",
    "\n",
    "iimage = Image.open(io.BytesIO(url.content))\n",
    "size = iimage.size\n",
    "# define transformt o resize the image with given size\n",
    "transform = T.Resize(size = (224, 224))\n",
    "\n",
    "# apply the transform on the input image\n",
    "image = transform(iimage)\n",
    "transform = transforms.ToTensor()\n",
    "tensor = transform(image).unsqueeze(0)\n",
    "img = np.array(image)\n",
    "tensor = tensor.reshape([ 224, 224,3])\n",
    "cg = Image.fromarray(img)\n",
    "cg = np.float32(cg) / 255\n",
    "#display(let)\n",
    "input_tensor = preprocess_image(cg, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "target_layers = [model.layer4]\n",
    "\n",
    "#model.cuda()\n",
    "#input_tensor = input_tensor.cuda()\n",
    "np.random.seed(42)\n",
    "benchmark(input_tensor, target_layers, eigen_smooth=False, aug_smooth=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f49b2ae5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [15], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#model.cuda()\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#input_tensor = input_tensor.cuda()\u001b[39;00m\n\u001b[0;32m     22\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m \u001b[43mbenchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maug_smooth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mcategory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m246\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [13], line 31\u001b[0m, in \u001b[0;36mbenchmark\u001b[1;34m(input_tensor, target_layers, eigen_smooth, aug_smooth, category)\u001b[0m\n\u001b[0;32m     28\u001b[0m     attributions \u001b[38;5;241m=\u001b[39m cam_method(input_tensor\u001b[38;5;241m=\u001b[39minput_tensor, \n\u001b[0;32m     29\u001b[0m                               targets\u001b[38;5;241m=\u001b[39mtargets, eigen_smooth\u001b[38;5;241m=\u001b[39meigen_smooth, aug_smooth\u001b[38;5;241m=\u001b[39maug_smooth)\n\u001b[0;32m     30\u001b[0m attribution \u001b[38;5;241m=\u001b[39m attributions[\u001b[38;5;241m0\u001b[39m, :]    \n\u001b[1;32m---> 31\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mcam_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m score \u001b[38;5;241m=\u001b[39m scores[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     33\u001b[0m visualization \u001b[38;5;241m=\u001b[39m show_cam_on_image(cg, attribution, use_rgb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Documents\\pytorch-grad-cam\\cams\\utils\\road.py:254\u001b[0m, in \u001b[0;36mROADCombined.__call__\u001b[1;34m(self, input_tensor, cams, targets, model)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    248\u001b[0m              input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    249\u001b[0m              cams: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[0;32m    250\u001b[0m              targets: List[Callable],\n\u001b[0;32m    251\u001b[0m              model: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m    253\u001b[0m     scores_lerf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlerf_averager(input_tensor, cams, targets, model)\n\u001b[1;32m--> 254\u001b[0m     scores_morf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmorf_averager\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (scores_lerf \u001b[38;5;241m-\u001b[39m scores_morf) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[1;32m~\\Documents\\pytorch-grad-cam\\cams\\utils\\road.py:109\u001b[0m, in \u001b[0;36mAveragerAcrossThresholds.__call__\u001b[1;34m(self, input_tensor, cams, targets, model)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m percentile \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpercentiles:\n\u001b[0;32m    108\u001b[0m     imputer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimputer(percentile)\n\u001b[1;32m--> 109\u001b[0m     scores\u001b[38;5;241m.\u001b[39mappend(\u001b[43mimputer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mfloat32(scores), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\Documents\\pytorch-grad-cam\\cams\\utils\\road.py:39\u001b[0m, in \u001b[0;36mPerturbationConfidenceMetric.__call__\u001b[1;34m(self, input_tensor, cams, targets, model, return_visualization, return_diff)\u001b[0m\n\u001b[0;32m     36\u001b[0m perturbated_tensors \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(perturbated_tensors)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 39\u001b[0m     outputs_after_imputation \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperturbated_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m scores_after_imputation \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     41\u001b[0m     target(output)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m target, output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[0;32m     42\u001b[0m         targets, outputs_after_imputation)]\n\u001b[0;32m     43\u001b[0m scores_after_imputation \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32(scores_after_imputation)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\models\\resnet.py:274\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    271\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[0;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[1;32m--> 274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n\u001b[0;32m    276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\models\\resnet.py:158\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    155\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(out)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     identity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m identity\n\u001b[0;32m    161\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    451\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    452\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "url = requests.get('https://www.southernliving.com/thmb/tXa6uF93OgesMpTj8UVX6HfMNZw=/750x0/filters:no_upscale():max_bytes(150000):strip_icc():format(webp)/GettyImages-185743593-2000-507c6c8883a44851885ea4fbc10a2c9e.jpg',stream=True)\n",
    "model = models.resnet50(pretrained=True)\n",
    "iimage = Image.open(io.BytesIO(url.content))\n",
    "size = iimage.size\n",
    "# define transformt o resize the image with given size\n",
    "transform = T.Resize(size = (204, 204))\n",
    "\n",
    "# apply the transform on the input image\n",
    "image = transform(iimage)\n",
    "transform = transforms.ToTensor()\n",
    "tensor = transform(image).unsqueeze(0)\n",
    "img = np.array(image)\n",
    "tensor = tensor.reshape([ 204, 204,3])\n",
    "cg = Image.fromarray(img)\n",
    "cg = np.float32(cg) / 255\n",
    "#display(let)\n",
    "input_tensor = preprocess_image(cg, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "target_layers = [model.layer4]\n",
    "\n",
    "#model.cuda()\n",
    "#input_tensor = input_tensor.cuda()\n",
    "np.random.seed(42)\n",
    "benchmark(input_tensor, target_layers, eigen_smooth=False, aug_smooth=False,category=246)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c935a5ad",
   "metadata": {},
   "source": [
    "predicting dog from the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa66a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = requests.get('https://wallpaper-house.com/data/out/8/wallpaper2you_272733.jpg',stream=True)\n",
    "model = models.resnet50(pretrained=True)\n",
    "iimage = Image.open(io.BytesIO(url.content))\n",
    "size = iimage.size\n",
    "# define transformt o resize the image with given size\n",
    "transform = T.Resize(size = (204, 204))\n",
    "\n",
    "# apply the transform on the input image\n",
    "image = transform(iimage)\n",
    "transform = transforms.ToTensor()\n",
    "tensor = transform(image).unsqueeze(0)\n",
    "img = np.array(image)\n",
    "tensor = tensor.reshape([ 204, 204,3])\n",
    "cg = Image.fromarray(img)\n",
    "cg = np.float32(cg) / 255\n",
    "#display(let)\n",
    "input_tensor = preprocess_image(cg, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "target_layers = [model.layer4]\n",
    "\n",
    "#model.cuda()\n",
    "#input_tensor = input_tensor.cuda()\n",
    "np.random.seed(42)\n",
    "benchmark(input_tensor, target_layers, eigen_smooth=False, aug_smooth=False,category=246)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c07ba5",
   "metadata": {},
   "source": [
    "predicting cat from the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c011b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = requests.get('https://wallpaper-house.com/data/out/8/wallpaper2you_272733.jpg',stream=True)\n",
    "model = models.resnet50(pretrained=True)\n",
    "iimage = Image.open(io.BytesIO(url.content))\n",
    "size = iimage.size\n",
    "# define transformt o resize the image with given size\n",
    "transform = T.Resize(size = (204, 204))\n",
    "\n",
    "# apply the transform on the input image\n",
    "image = transform(iimage)\n",
    "transform = transforms.ToTensor()\n",
    "tensor = transform(image).unsqueeze(0)\n",
    "img = np.array(image)\n",
    "tensor = tensor.reshape([ 204, 204,3])\n",
    "cg = Image.fromarray(img)\n",
    "cg = np.float32(cg) / 255\n",
    "#display(let)\n",
    "input_tensor = preprocess_image(cg, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "target_layers = [model.layer4]\n",
    "\n",
    "#model.cuda()\n",
    "#input_tensor = input_tensor.cuda()\n",
    "np.random.seed(42)\n",
    "benchmark(input_tensor, target_layers, eigen_smooth=False, aug_smooth=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6815d4",
   "metadata": {},
   "source": [
    "Resnet34 model and [model.layer4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cddd1ed",
   "metadata": {},
   "source": [
    "predicting dog from the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e151f7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = requests.get('https://wallpaper-house.com/data/out/8/wallpaper2you_272733.jpg',stream=True)\n",
    "model = models.resnet34(pretrained=True)\n",
    "iimage = Image.open(io.BytesIO(url.content))\n",
    "size = iimage.size\n",
    "# define transformt o resize the image with given size\n",
    "transform = T.Resize(size = (204, 204))\n",
    "\n",
    "# apply the transform on the input image\n",
    "image = transform(iimage)\n",
    "transform = transforms.ToTensor()\n",
    "tensor = transform(image).unsqueeze(0)\n",
    "img = np.array(image)\n",
    "tensor = tensor.reshape([ 204, 204,3])\n",
    "cg = Image.fromarray(img)\n",
    "cg = np.float32(cg) / 255\n",
    "#display(let)\n",
    "input_tensor = preprocess_image(cg, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "target_layers = [model.layer4]\n",
    "\n",
    "#model.cuda()\n",
    "#input_tensor = input_tensor.cuda()\n",
    "np.random.seed(42)\n",
    "benchmark(input_tensor, target_layers, eigen_smooth=False, aug_smooth=False,category=246)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d37f122",
   "metadata": {},
   "source": [
    "predicting cat from the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04f9569",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = requests.get('https://wallpaper-house.com/data/out/8/wallpaper2you_272733.jpg',stream=True)\n",
    "model = models.resnet34(pretrained=True)\n",
    "iimage = Image.open(io.BytesIO(url.content))\n",
    "size = iimage.size\n",
    "# define transformt o resize the image with given size\n",
    "transform = T.Resize(size = (204, 204))\n",
    "\n",
    "# apply the transform on the input image\n",
    "image = transform(iimage)\n",
    "transform = transforms.ToTensor()\n",
    "tensor = transform(image).unsqueeze(0)\n",
    "img = np.array(image)\n",
    "tensor = tensor.reshape([ 204, 204,3])\n",
    "cg = Image.fromarray(img)\n",
    "cg = np.float32(cg) / 255\n",
    "#display(let)\n",
    "input_tensor = preprocess_image(cg, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "target_layers = [model.layer4]\n",
    "\n",
    "#model.cuda()\n",
    "#input_tensor = input_tensor.cuda()\n",
    "np.random.seed(42)\n",
    "benchmark(input_tensor, target_layers, eigen_smooth=False, aug_smooth=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce4fb2b",
   "metadata": {},
   "source": [
    "Resnet50 model and [model.layer3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75420049",
   "metadata": {},
   "source": [
    "predicting dog from the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a59d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = requests.get('https://wallpaper-house.com/data/out/8/wallpaper2you_272733.jpg',stream=True)\n",
    "model = models.resnet50(pretrained=True)\n",
    "iimage = Image.open(io.BytesIO(url.content))\n",
    "size = iimage.size\n",
    "# define transformt o resize the image with given size\n",
    "transform = T.Resize(size = (204, 204))\n",
    "\n",
    "# apply the transform on the input image\n",
    "image = transform(iimage)\n",
    "transform = transforms.ToTensor()\n",
    "tensor = transform(image).unsqueeze(0)\n",
    "img = np.array(image)\n",
    "tensor = tensor.reshape([ 204, 204,3])\n",
    "cg = Image.fromarray(img)\n",
    "cg = np.float32(cg) / 255\n",
    "#display(let)\n",
    "input_tensor = preprocess_image(cg, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "target_layers = [model.layer3]\n",
    "\n",
    "#model.cuda()\n",
    "#input_tensor = input_tensor.cuda()\n",
    "np.random.seed(42)\n",
    "benchmark(input_tensor, target_layers, eigen_smooth=False, aug_smooth=False,category=246)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbaf107",
   "metadata": {},
   "source": [
    "predicting cat from the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f093d0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = requests.get('https://wallpaper-house.com/data/out/8/wallpaper2you_272733.jpg',stream=True)\n",
    "model = models.resnet50(pretrained=True)\n",
    "iimage = Image.open(io.BytesIO(url.content))\n",
    "size = iimage.size\n",
    "# define transformt o resize the image with given size\n",
    "transform = T.Resize(size = (204, 204))\n",
    "\n",
    "# apply the transform on the input image\n",
    "image = transform(iimage)\n",
    "transform = transforms.ToTensor()\n",
    "tensor = transform(image).unsqueeze(0)\n",
    "img = np.array(image)\n",
    "tensor = tensor.reshape([ 204, 204,3])\n",
    "cg = Image.fromarray(img)\n",
    "cg = np.float32(cg) / 255\n",
    "#display(let)\n",
    "input_tensor = preprocess_image(cg, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "target_layers = [model.layer3]\n",
    "\n",
    "#model.cuda()\n",
    "#input_tensor = input_tensor.cuda()\n",
    "np.random.seed(42)\n",
    "benchmark(input_tensor, target_layers, eigen_smooth=False, aug_smooth=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcbc018",
   "metadata": {},
   "source": [
    "Resnet34 model and [model.layer3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cae10e",
   "metadata": {},
   "source": [
    "predicting dog from the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d88fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = requests.get('https://wallpaper-house.com/data/out/8/wallpaper2you_272733.jpg',stream=True)\n",
    "model = models.resnet34(pretrained=True)\n",
    "iimage = Image.open(io.BytesIO(url.content))\n",
    "size = iimage.size\n",
    "# define transformt o resize the image with given size\n",
    "transform = T.Resize(size = (204, 204))\n",
    "\n",
    "# apply the transform on the input image\n",
    "image = transform(iimage)\n",
    "transform = transforms.ToTensor()\n",
    "tensor = transform(image).unsqueeze(0)\n",
    "img = np.array(image)\n",
    "tensor = tensor.reshape([ 204, 204,3])\n",
    "cg = Image.fromarray(img)\n",
    "cg = np.float32(cg) / 255\n",
    "#display(let)\n",
    "input_tensor = preprocess_image(cg, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "target_layers = [model.layer3]\n",
    "\n",
    "#model.cuda()\n",
    "#input_tensor = input_tensor.cuda()\n",
    "np.random.seed(42)\n",
    "benchmark(input_tensor, target_layers, eigen_smooth=False, aug_smooth=False,category=246)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1f3b25",
   "metadata": {},
   "source": [
    "predicting cat from the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4357aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = requests.get('https://wallpaper-house.com/data/out/8/wallpaper2you_272733.jpg',stream=True)\n",
    "model = models.resnet34(pretrained=True)\n",
    "iimage = Image.open(io.BytesIO(url.content))\n",
    "size = iimage.size\n",
    "# define transformt o resize the image with given size\n",
    "transform = T.Resize(size = (204, 204))\n",
    "\n",
    "# apply the transform on the input image\n",
    "image = transform(iimage)\n",
    "transform = transforms.ToTensor()\n",
    "tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "img = np.array(image)\n",
    "tensor = tensor.reshape([ 204, 204,3])\n",
    "\n",
    "cg = Image.fromarray(img)\n",
    "cg = np.float32(cg) / 255\n",
    "#display(let)\n",
    "input_tensor = preprocess_image(cg, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "target_layers = [model.layer3]\n",
    "\n",
    "#model.cuda()\n",
    "#input_tensor = input_tensor.cuda()\n",
    "np.random.seed(42)\n",
    "benchmark(input_tensor, target_layers, eigen_smooth=False, aug_smooth=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535058e9",
   "metadata": {},
   "source": [
    "densenet121 and denseblock4 layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed3ef7a",
   "metadata": {},
   "source": [
    "predicting dog from the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d36ec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = requests.get('https://wallpaper-house.com/data/out/8/wallpaper2you_272733.jpg',stream=True)\n",
    "model = models.densenet121(pretrained=True)\n",
    "iimage = Image.open(io.BytesIO(url.content))\n",
    "size = iimage.size\n",
    "# define transformt o resize the image with given size\n",
    "transform = T.Resize(size = (204, 204))\n",
    "\n",
    "# apply the transform on the input image\n",
    "image = transform(iimage)\n",
    "transform = transforms.ToTensor()\n",
    "tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "img = np.array(image)\n",
    "tensor = tensor.reshape([ 204, 204,3])\n",
    "\n",
    "cg = Image.fromarray(img)\n",
    "cg = np.float32(cg) / 255\n",
    "#display(let)\n",
    "input_tensor = preprocess_image(cg, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "target_layers = [model.features.denseblock4]\n",
    "\n",
    "#model.cuda()\n",
    "#input_tensor = input_tensor.cuda()\n",
    "np.random.seed(42)\n",
    "benchmark(input_tensor, target_layers, eigen_smooth=False, aug_smooth=False,category=246)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ebf05e",
   "metadata": {},
   "source": [
    "predicting cat from the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a5501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = requests.get('https://wallpaper-house.com/data/out/8/wallpaper2you_272733.jpg',stream=True)\n",
    "model = models.densenet121(pretrained=True)\n",
    "iimage = Image.open(io.BytesIO(url.content))\n",
    "size = iimage.size\n",
    "# define transformt o resize the image with given size\n",
    "transform = T.Resize(size = (204, 204))\n",
    "\n",
    "# apply the transform on the input image\n",
    "image = transform(iimage)\n",
    "transform = transforms.ToTensor()\n",
    "tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "img = np.array(image)\n",
    "tensor = tensor.reshape([ 204, 204,3])\n",
    "\n",
    "cg = Image.fromarray(img)\n",
    "cg = np.float32(cg) / 255\n",
    "#display(let)\n",
    "input_tensor = preprocess_image(cg, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "target_layers = [model.features.denseblock4]\n",
    "\n",
    "#model.cuda()\n",
    "#input_tensor = input_tensor.cuda()\n",
    "np.random.seed(42)\n",
    "benchmark(input_tensor, target_layers, eigen_smooth=False, aug_smooth=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6559046",
   "metadata": {},
   "source": [
    "densenet121 and denseblock3 layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b351e5bb",
   "metadata": {},
   "source": [
    "predicting dog from the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a105b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = requests.get('https://wallpaper-house.com/data/out/8/wallpaper2you_272733.jpg',stream=True)\n",
    "model = models.densenet121(pretrained=True)\n",
    "iimage = Image.open(io.BytesIO(url.content))\n",
    "size = iimage.size\n",
    "# define transformt o resize the image with given size\n",
    "transform = T.Resize(size = (204, 204))\n",
    "\n",
    "# apply the transform on the input image\n",
    "image = transform(iimage)\n",
    "transform = transforms.ToTensor()\n",
    "tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "img = np.array(image)\n",
    "tensor = tensor.reshape([ 204, 204,3])\n",
    "\n",
    "cg = Image.fromarray(img)\n",
    "cg = np.float32(cg) / 255\n",
    "#display(let)\n",
    "input_tensor = preprocess_image(cg, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "target_layers = [model.features.denseblock3]\n",
    "\n",
    "#model.cuda()\n",
    "#input_tensor = input_tensor.cuda()\n",
    "np.random.seed(42)\n",
    "benchmark(input_tensor, target_layers, eigen_smooth=False, aug_smooth=False,category=246)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b758155c",
   "metadata": {},
   "source": [
    "predicting cat from the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac68cc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = requests.get('https://wallpaper-house.com/data/out/8/wallpaper2you_272733.jpg',stream=True)\n",
    "model = models.densenet121(pretrained=True)\n",
    "iimage = Image.open(io.BytesIO(url.content))\n",
    "size = iimage.size\n",
    "# define transformt o resize the image with given size\n",
    "transform = T.Resize(size = (204, 204))\n",
    "\n",
    "# apply the transform on the input image\n",
    "image = transform(iimage)\n",
    "transform = transforms.ToTensor()\n",
    "tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "img = np.array(image)\n",
    "tensor = tensor.reshape([ 204, 204,3])\n",
    "\n",
    "cg = Image.fromarray(img)\n",
    "cg = np.float32(cg) / 255\n",
    "#display(let)\n",
    "input_tensor = preprocess_image(cg, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "target_layers = [model.features.denseblock3]\n",
    "\n",
    "#model.cuda()\n",
    "#input_tensor = input_tensor.cuda()\n",
    "np.random.seed(42)\n",
    "benchmark(input_tensor, target_layers, eigen_smooth=False, aug_smooth=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cebbef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d366b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aed0a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
